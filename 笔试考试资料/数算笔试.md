# 数算笔试

## 1 Understanding large language models

### 1.1 什么是 LLM（Large Language Model）

- **定义**：LLM 是在海量文本上训练的**深度神经网络**，用于理解、生成、回答自然语言。
- **核心训练任务**：通常用“**预测下一个词/下一个 token**”（next-token prediction）学习语言的**上下文、结构、语义关系**。
- **关键架构**：多使用 **Transformer**，依靠**注意力机制**在输入序列中挑重点信息（对输入的不同部分分配注意力），擅长处理长距离依赖与语言细节。
- **“Large” 的含义**：
    - **参数规模**：大量可调权重（parameters）。
    - **数据规模**：训练集极大且多样。
- **与生成式 AI**（Generative AI）**的关系**：
    - 生成式AI：涵盖能产生新内容的AI系统
    - LLM 属于**生成式 AI，因为能够生成文本**。
    - 生成式 AI 的范围更广：文本、图像、音乐等都包括。
- **AI / ML / DL 关系**：
    - AI：是更大领域，目标是创造能够执行需要类人智能的任务的机器
    - 机器学习：AI的一个子领域，强调从数据中学习的算法
    - 深度学习：是机器学习的一个分支，使用多层的深度神经网络来构建数据中的复杂模式
    - AI（人工智能）> ML（机器学习）> DL（深度学习）。
- **传统 ML vs 深度学习**：
    - 传统 ML 常需人工特征工程；
    - 深度学习可从数据中自动学习特征表示。
- **概念层级连接（记忆版）**：LLM ⊂ 生成式 AI ⊂（更广的）AI 体系；LLM 是深度学习模型的一种典型应用。

### 1.2 LLM 的应用（Applications）

- **常见应用场景**：
    - 机器翻译、文本生成、情感分析、文本摘要、内容创作
    - 聊天机器人/虚拟助手（如 ChatGPT、Gemini）
    - 专业领域知识检索与辅助（医学、法律等）：长文总结、技术问答、信息提取
- **总体价值**：让技术更“可对话”、更直观、更易用，通过自然语言完成大量文本任务自动化。

### 1.3 构建与使用 LLM 的阶段（Stages）

- **为什么做定制 LLM（优势）**：
    - 在特定任务/领域表现更好
    - **隐私与合规**更可控（减少对第三方依赖）
    - 可本地部署：更低延迟、更低长期成本
- **基础流程（两段式）**：
    - **预训练 Pretraining**：用海量多样数据学“通用语言能力”（形成基础模型）
        - 目标：得到一个对语言具有普遍理解能力的基础模型，使用大量未标注文本（原始文本 raw text），训练模型去预测序列中的下一个词。
    - **微调 Fine-tuning**：用更小、更具体数据适配任务/领域
- **预训练的学习方式**：**自监督学习**（self-supervised）
    - 标签由数据自身构造（例如下一个词就是标签），无需人工标注
- **微调的两类常见形式**：
    - **指令微调**：instruction → answer 配对 指令—回答
    - **分类微调**：text → class label 配对 文本—类别标签

### 1.4 Transformer 架构（Transformer Basics）

Transformer 架构 介绍：

https://www.zhihu.com/tardis/bd/art/600773858?source_id=1001

- **地位**：是一种革新自然语言处理的深度神经网络架构，多数现代 NLP/LLM 的核心基础架构之一。
- **基本组成**：Encoder（编码器）+ Decoder（解码器）
    - Encoder：把输入文本编码为数值表示
        - 输入层：embedding层把文本编码变为数值表示
    - Decoder：基于表示生成输出文本
- **自注意力 Self-Attention**：
    - 使Transformer能根据上下文为序列中不同词分配不同重要性权重
    - 能捕捉长距离依赖，使输出更连贯、更贴合上下文
- **BERT vs GPT（对比记忆点）**：
    - **BERT**：更偏理解/判别任务（如分类），典型训练是**掩码词预测**
    - **GPT**：更偏生成任务（续写、翻译、摘要等），典型训练是**下一个词预测**

- **零样本/少样本能力**：
    - **Zero-shot**：不给示例也能做任务
    - **Few-shot**：给少量示例即可学会任务模式
    - 体现了 GPT 的灵活性与适应性

### 1.5 大规模数据与 Token（Utilizing large datasets）

- **训练数据特点**：规模巨大（数十亿词级别）、主题与语言多样
- → 让模型学到语法、语义、语境与常识性规律。
- **Tokenization（分词/切分）**：
    - 把文本转成 **tokens**（模型读取的基本单位）
    - token 不一定等于“一个词”，也可能是子词、标点等
- **预训练 vs 微调（再强调）**：
    - 预训练：学通用规律 → 得到基础模型
    - 微调：用小而专的数据 → 强化某一任务表现

### 1.6 GPT 架构细看（GPT Architecture）

- **训练核心**：仍是 **next-token prediction 下一个词预测**
- → 学到词/短语关系后，会出现“迁移能力”（如没专门训翻译也能做翻译）。
- **学习方式**：自监督（下一个词作为标签）
- **结构特点**：**Decoder-only Transformer**
    - 单向（左→右）逐词生成
    - Decoder only 适合文本生成与下一个词预测
- **自回归 Autoregressive**：
    - 生成时把“前一步输出”作为“下一步输入”
    - 保障连贯性与流畅性
- **规模效应**：更深层数/更多参数（如 GPT-3 相比原始 Transformer 更大）

→ 往往带来更强泛化与更多任务能力。

### 1.7 构建一个 LLM（Build an LLM）

- **三大阶段（总框架）**：
    - 架构实现 + 数据准备
    - 预训练 → 得到基础模型
    - 微调 → 对基础模型进行微调，适配具体任务
- **Transformer 注意力的意义**：生成每个词时都能访问并利用整个输入序列（选择性关注关键信息）。
- **（GPT类模型）涌现能力 Emergent abilities**：
    - 虽然主要训的是 next-token prediction
    - 但会“自然涌现”出分类、翻译、摘要等能力
- **微调的作用**：让模型在特定领域/任务更专业，有时能超过通用模型。

## 2 Working with text data

- 本章目标：把文本数据变成 LLM 可训练的**数值输入**，核心是把文本转为 **embeddings（嵌入向量）**。
- 流程主线： **Text → Tokenization（分词）→ Vocabulary（词表）→ Token IDs → Token Embeddings → Positional Embeddings → Input Embeddings → 送入主模型层**
- Tokenization：把文本切成**单词或子词**等单位，再映射为 ID。
- 覆盖 BPE（Byte Pair Encoding），GPT 系列使用的分词方案之一。
- 最终目的：构造“token 向量 + 位置向量”的输入，为后续 LLM 模块提供输入张量。

### 2.1 Understanding word embeddings（词嵌入）

**核心概念**

- 深度学习模型处理的是**数值数据**；文本是离散/类别型符号 → 必须转成连续数值向量才能计算。
    - **Word embeddings（词嵌入）**：把词转换为**连续向量**，使其适配神经网络的数学运算。

**Word2Vec 原理**

- 训练神经网络去：
    - 预测“给定目标词的上下文”，或
    - 预测“给定上下文的目标词”
- 基本假设：**相似上下文出现的词语意义相近** → 在嵌入空间中会聚在一起（clustered representations）。

**嵌入维度（Embedding dimensionality）权衡**

- 维度更高：表达更细腻的语义关系，但**计算开销更大**。
- 维度更低：**更快更省算力**，但可能损失语义细节。

**LLM 与 Word2Vec 的区别**

- LLM 通常在模型内部（输入层）**自行学习 embeddings**，并在训练中持续优化 → embeddings 更贴合任务与数据，可能优于直接使用 Word2Vec 等预训练嵌入。

**可视化挑战**

- 人类常规可视化直觉≤3维，高维嵌入需要**特殊方法/降维技术**才能可视化。

### 2.2 Tokenizing text（分词/标记化）

**目的**

- Tokenization 是创建 llm embeddings 前的关键预处理：把文本拆成 tokens（单词、标点、特殊字符等），为后续建立词表、生成 ID、构造嵌入做准备。

**用 Python 正则 re 的分词思路**

- 使用 re.split 通过正则模式拆分文本：可匹配

**空白字符、标点符号、其他特殊字符**

- 拆分后得到 token 列表；通常还需要进一步处理以**去除多余/冗余的空白 token**。

**大小写（capitalization）为什么重要**

- 保留大小写有助于模型：
    - 区分专有名词 vs 普通名词
    - 理解句子结构
    - 学会生成符合规范的大小写文本

→ 因此训练有效 LLM 时通常**保留大小写**更有利。

**空白处理的权衡**

- 去空白：降低内存与计算成本。
- 保留空白：对“结构敏感文本”很重要，例如 **Python 代码**依赖缩进与空格（空白本身携带语法/结构信息）。

### 2.3 Converting tokens into token IDs（token → token ID）

**为什么要转成 token IDs**

- token IDs 是把 token 转为 embedding vectors 前的**中间表示**：整数序列更紧凑、更高效，便于语言模型处理。

**词表（vocabulary）如何创建**

- 对**整个训练集**进行 tokenization
- 收集**唯一 tokens**
- 按字母序排序
- 给每个 token 分配一个唯一整数（形成 token→id 映射）

**SimpleTokenizerV1：encode / decode**

- encode(text)：文本 → tokens → 用词表映射为 **token IDs（整数序列）**
- decode(token_ids)：token IDs → 用反向词表映射回 tokens → 还原可读文本

**小训练集词表的局限**

- 训练集太小 → 词表覆盖不足
- 遇到训练中没出现的新词/短语会导致 tokenization/decoding 问题

→ 强调训练集应**大且多样**以构建稳健 tokenizer。

### 2.4 Adding special context tokens（添加特殊上下文 token）

**新增的两个特殊 token 及作用**

1.  &lt;|unk|&gt;：表示词表中没有的**未知词**（unknown word / OOV）
2.  &lt;|endoftext|&gt;：用于分隔训练时拼接的**互不相关文本来源/独立文档**，帮助模型理解边界与“这是不同文本”。

**SimpleTokenizerV2 如何处理未知词**

- 遇到词表不存在的词（OOV词） → 用 &lt;|unk|&gt; 替换
- 这样保证编码结果始终是可表示的 token IDs 序列。

**多独立文档训练时 &lt;|endoftext|&gt; 的意义**

- 作为片段边界标记：提示模型“虽然拼接了，但这些段落/文档彼此独立”。

**常见额外特殊 tokens 与功能**

- \[BOS\]：beginning of sequence（序列开始）
- \[EOS\]：end of sequence（序列结束）
- \[PAD\]：padding（补齐 batch 内长度）

### 2.5 Byte pair encoding（BPE）

**BPE 的核心优势（尤其对未知词）**

- 遇到未知词时不用 &lt;|unk|&gt;，而是把词拆成更小的**子词单元（subword units）或字符**
- 任何新词都能表示为“已知子词/字符 token 序列”。

**GPT-2 / GPT-3 / 原始 ChatGPT 的 BPE 词表大小**

- 文档给出的数值：**50,257**
- &lt;|endoftext|&gt; 被分配了**最大的 token ID**。

**BPE 如何处理 unknown words（如 someunknownPlace）**

- 通过拆分成子词或字符片段，使之完全由词表内单位组成
- 因此无需 &lt;|unk|&gt; 也能编码任意字符串。

**实现库**

- 文档指出使用：**tiktoken**（开源 Python 库；基于 Rust 的高效实现）

### 2.6 Data sampling with a sliding window（滑动窗口采样）

**为什么需要 input-target pairs**

- 训练目标是 next-token prediction：
    - **Input**：上下文 token 序列
    - **Target**：紧接着的下一个 token
- input-target pairs 给模型“输入序列 + 对应下一词目标”，让模型学会在给定上下文下预测最可能的下一个词/token

**滑动窗口（sliding window）生成方式**

- 在长 token 序列上移动窗口，截取**输入块**
- 每个输入块配对相应“下一 token”（或等长右移序列）作为 target
- 通过不断滑动产生大量（通常有重叠的）训练样本。

**stride 参数**

- 决定窗口每次移动的步长：
    - stride 小：重叠多 → 样本更多
    - stride 大：重叠少 → 样本更少
- 影响训练数据数量与上下文覆盖方式（以及捕捉长程依赖的机会）。

**max_length 参数**

- 决定每个输入块包含多少 token（上下文长度）：
    - max_length 大：上下文更长，但计算成本更高
    - max_length 小：更省算力，但上下文信息更少

**PyTorch Dataset / DataLoader 的意义（选择题那题）**

- 用于高效管理与迭代大规模数据：支持
    - batching（成批）
    - shuffling（打乱）
    - parallel loading（并行加载）
- 对 LLM 训练效率非常关键。
- 四个定义要记清：
    - **BPE**：把词拆成子词/字符，能处理未知词的 tokenization 方案
    - **Subword units**：BPE 拆出来的更小单位（字符或字符组合）
    - **OOV words**：不在 tokenizer 预定义词表中的词
    - **Vocabulary size**：tokenizer 能识别的唯一 token 总数

### 2.7 Creating token embeddings（创建 token 嵌入）

**为什么需要 embedding vectors**

- GPT 类 LLM 是深度网络，靠反向传播学习；需要连续向量作为输入表示 → embeddings 是必要的。

**embedding 权重如何初始化**

- 一开始通常是**随机初始化**
- 训练中通过反向传播不断优化（学习到有语义的向量空间）。

**embedding layer 如何把 token ID 变成向量**

- embedding layer 是查表（lookup）：

输入 token ID → 在权重矩阵中取对应行 → 得到该 token 的 embedding vector**权重矩阵与 vocab_size / embedding_dim 的关系**

- 权重矩阵形状：
    - 行数 = **vocabulary size**（每个 token 一行）
    - 列数 = **embedding dimension**（向量维度）
- 四个概念定义必须背：
    - **Context size**：模型用于预测下一个词的输入 token 数（上下文长度）
    - **Input-target pairs**：输入序列 + 下一个 token 目标
    - **Sliding window**：用滑动窗口从文本中构造 input-target pairs 的方法
    - **Stride**：窗口每次移动的 token 数（步长）

### 2.8 Encoding word positions（编码 token 位置）

**LLM 在顺序上的缺陷**

- 自注意力机制本身**不天然包含 token 顺序信息**（“缺少 token order 概念”）。

**解决方案：positional embeddings**

- 给每个位置提供向量，使模型知道 token 的顺序与位置。

**绝对 vs 相对位置嵌入**

- **绝对位置嵌入**：每个位置一个独立向量，编码“精确位置”
- **相对位置嵌入**：编码 token 之间的相对距离，更易泛化到不同长度序列

**OpenAI GPT 模型怎么做**

- 使用**可训练的绝对位置嵌入，**并且这些位置嵌入是**可训练参数**，随训练共同优化（不是固定的手工设定）。

**input embeddings 的构造（必须会写）**

- 先 token IDs → token embeddings
- 再为每个位置生成 positional embeddings
- 两者**相加**：input_embedding = token_embedding + positional_embedding

**代码中两层的作用**

- token_embedding_layer：token ID → token embedding vector
- pos_embedding_layer：位置索引 → positional embedding vector

## 3 Coding Attention Mechanisms

- 本章目标：实现 LLM（尤其 GPT）中的注意力机制代码；重点是 **self-attention**。
- Self-attention 定义：序列中每个位置在计算自身表示时，可以对同序列中其他所有位置分配注意力（相关性权重）。
- GPT 系列（Transformer 架构 LLM）依赖自注意力作为核心组件。

### 3.1 长序列建模问题

The problem with modeling long sequences

- 挑战：（翻译难点）不同语言**语法结构/词序不同**，逐词翻译会丢意义与语境。

传统序列模型：RNN

- Encoder：读完整输入，把意义编码浓缩为 **hidden state**。
- Decoder：用 hidden state **逐词生成**输出译文。
- Encoder-decoder RNN 局限：解码依赖“当前 hidden state”，长距离依赖时易**丢上下文**。
- Hidden state 定义：输入序列意义的**压缩表示/记忆单元**，供 decoder 使用。

### 3.2 Capturing data dependencies with attention mechanisms（注意力捕捉依赖）

- RNN 长文本翻译的根本限制：必须把全部输入记在**单一 hidden state**，前面信息容易丢。
- 自注意力意义：Transformer-LMM（GPT）关键组件，能捕捉**长程依赖**并建模词间关系。
- 术语配对：
    - Attention Mechanism（注意力机制）：在生成/理解序列时，不把全部信息压缩成一个固定向量，而是在每一步“有选择地”从输入（或上下文）里取出最相关的部分进行计算。
    - Self-Attention：序列内元素彼此都能注意到

- - Transformer Architecture：依赖自注意力的架构，擅长长依赖
    - GPT Series：基于 Transformer 的大模型家族

### 3.3 Attending to different parts of the input with self-attention（自注意力关注输入）

在序列任务（翻译、理解、生成）里，**一个 token 的含义往往取决于同一句子里其他 token**（指代、修饰、主谓宾依赖、否定词等）。

- self-attention 的核心思想是：**在“同一序列内部”，让每个位置都能根据相关性，动态地聚合其它位置的信息。**
- **Q/K/V 都来自同一个输入序列，序列内部各位置互相关联，学习内部依赖。**
- self attention中 “self” 含义：在**同一序列内部**各位置互相关联，学习序列内部依赖。

三个关键对象：score -- weights -- context （最重要的逻辑链）

- Attention scores（未归一化相似度）：query 与各元素的相似度中间值（点积计算）。
    - 代码缺失函数：dot()（计算这个点积）
- Normalization（归一化） -- Attention weights（可解释权重）
    - 目的：得到全为正数，和为 1 的 attention weights，这样每个weight就像注意力分配比例，便于解释并增强训练稳定性。
    - 最常用的归一化：softmax
        - Softmax作用：把scores 归一化为**全正且和为1**的权重，更稳定、可解释。
- **scores（原始相关性）→ softmax → weights（注意力权重）**
- Context vector：融合所有元素信息后的**增强嵌入表示**。
    - 计算方式：加权和 向量\*权重 后求和
    - 第i个token不再只代表自己，而是把与自己相关的全句信息 揉进来了，所以叫enhanced embedding / context-aware representation。
- 总结公式链：dot() → attention scores → softmax normalization → attention weights → weighted sum → context vector

### 3.4可训练权重的 Self-Attention 总结（Trainable Self-Attention）

1) 为什么引入 WQ,WK,WV（权重矩阵）

- 在 3.3 的简化 self-attention 里，很多时候默认 Q=K=V=X（直接用输入向量做相似度与加权和）。
- 3.4 进一步升级：引入可学习的投影矩阵：
    - Q=XWQ, K=XWK, V=XWV
- 作用：
    - 让模型自己学到“什么特征用来匹配”（Q/K 空间）
    - 以及“哪些信息应该被汇总传递”（V 空间）
    - 所以它不只是做相似度计算，而是在学习关系与重要性（谁该关注谁、该拿走什么信息）

2) 计算流程（你给的三步就是主线）

3) Scaled dot-product：为什么要除以 （step1：根号dk）

- 当 dk 维度变大时，点积 q⋅k的数值范围往往变大 →

softmax 输入变得极端 → 权重接近 one-hot → 梯度变小/训练不稳定（“softmax 饱和”）。

- 因此做缩放：（得到最后公式）
- 目的：
    - 让 softmax 的输入落在更合适范围
    - 避免饱和，梯度更健康
    - 训练更稳定、更有效

4) 代码缺失项答案（Position 1 2 3 → C E B）

Position 1 = C = W_value

Position 2 = E = torch

Position 3 = B = dim=-1（softmax 沿最后一维归一化）

记忆：softmax 的 dim=-1 通常表示“对每个 query 的所有 key 分数做归一化”。

5) SelfAttention_v1 vs SelfAttention_v2（实现差异与为什么 v2 更好）

- SelfAttention_v1
    - 用 nn.Parameter 手动创建并初始化 WQ,WK,WV
    - 灵活但容易初始化不佳，训练稳定性更依赖你自己写的初始化策略
- SelfAttention_v2
    - 用 nn.Linear 来实现投影（等价于可学习矩阵乘法 + 可选 bias）
    - 优点：
    - PyTorch 提供更成熟的默认初始化
    - 实现更标准、更不易出错
    - 通常训练更稳定，工程上更推荐

### 3.5 Hiding future words with causal attention 总结（Causal / Masked Attention）因果注意力

1) causal attention 是什么（核心定义）

- Causal attention（= masked attention）：在处理任一 token 时，只能关注过去 + 当前，不能关注未来 token。
- 它与标准 self-attention 的区别是：
    - 标准 self-attention：可以看全序列（过去/现在/未来）
    - causal attention：只能看过去与当前（用于生成任务，防止“偷看答案”）

2) Figure 3.19 的含义（必须记住的图像逻辑）

注意力矩阵中 对角线以上对应 “未来位置”

在 causal attention 中，这些位置会被 mask 掉

3) mask 怎么用（实现思路）

- mask 的目标：阻止模型给未来 token 分配注意力。
- 常见做法（概念层面）：
    - “对角线上方元素置零”来阻止关注未来 token

但注意：仅仅置零并不总是等价于真正“无影响”，因为 softmax 会把所有分数一起参与归一化。

- 更稳妥的解决方式是：
    - 在 softmax 之前，把被 mask 的位置设置成 −∞-\\infty−∞（或非常小的负数）
    - softmax 后这些位置权重变成 0
    - 这样被 mask 的位置对结果真正零影响

mask 后重新归一化权重，使 mask 位置影响为零。

本质就是：保证 mask 位置在 softmax 后权重为 0。

4) dropout 在 attention 里做什么、放在哪里

- dropout：防止过拟合。
- 在注意力里通常放在 attention weights 上（softmax 之后）：
    - 随机把部分权重置零
    - 同时对剩余权重做缩放（保持期望不变）
- 直觉：让模型不要过度依赖某几个固定位置。

5) register_buffer：为什么要用

- 在 PyTorch 里，causal mask 是一个固定张量（不是可训练参数），但需要随着模型一起移动设备：
    - .to(device) / .cuda() 时自动跟随
    - 避免 device mismatch（mask 在 CPU，数据在 GPU 的报错）
- 所以用 register_buffer 来注册 causal mask。

### 3.6 Extending single-head attention to multi-head attention 总结（Multi-Head Attention）(多头注意力）

1) 为什么要多头注意力（目的）

- 多头注意力 = 用不同投影从不同“视角”并行注意，从而捕捉更复杂的模式与关系。
- 直觉：
    - 单头：只能用一种相似性空间去关注
    - 多头：每个 head 学到不同类型关系（语法依赖、指代、局部搭配、长程主题等）

2) Wrapper 实现 vs MultiHeadAttention（结构差异）

- Wrapper 实现
    - 创建多个 CausalAttention（每个 head 一个独立模块）
    - 各 head 输出后 concatenate（拼接）
    - 特点：简单直观，但计算上会重复很多操作。
- MultiHeadAttention（单类集成多头）
    - 先用一次线性层得到 Q/K/V（一次投影）
    - reshape 拆成多个 head
    - 每个 head 做注意力
    - 再把 head 合并回来
    - 特点：更标准、更高效（工程上常用）。

3) 输出投影层（output projection layer）

- 多头输出拼接后维度通常是 h×dhead
- 输出投影层把它映射回原 embedding 维度：
    - 不是“数学上绝对必须”
    - 但在 LLM/Transformer 中非常常见（几乎标配）
    - 作用：把多头信息重新混合、统一回模型维度，方便残差连接等结构匹配

4) MultiHeadAttention 为什么更高效（关键原因）

- MultiHeadAttention：Q/K/V 的矩阵乘法只做一次，然后 reshape 分头
- Wrapper：每个 head 都要重复一次投影/计算 → 开销更大
- 所以标准实现更快、更省资源。

## 4 Implementing a GPT model from scratch to generate text

Tokenized text（分词后的 token id）

→ Embedding layers（token embedding + position embedding） 把离散 id 变成连续向量

→ Transformer blocks（多层） 反复做“上下文融合 + 非线性变换”

→ final LayerNorm 稳定输出分布

→ Output layer 投影到 vocab 得到 logits

→ （生成时）softmax→采样/argmax 得到下一 token

→ decode 回文本

### 4.1 编写 GPT 架构：组件、参数、配置、占位模型

图示标签（必须记）

1 输出层（Output layers）

2 嵌入层（Embedding layers）

3 分词文本（Tokenized text）

- GPT 模型目的（做什么）
    - GPT 是一个自回归语言模型：
    - 从海量文本中学习语言模式，然后在给定上下文时，逐 token 预测下一个 token，从而生成新文本。
- 核心组件与功能链路（为什么要这三大块）
    - Tokenized text：把文本变成可计算的离散 token id
    - Embedding layers：把 token id 变成神经网络可处理的连续向量（数值表示）
    - Transformer blocks：让每个 token 的表示融合上下文（学到词间关系、长程依赖）
    - Output layer：把每个位置的隐藏表示映射到词表维度，输出 logits（用于预测下一个 token）
- Parameters（参数）是什么
    - Parameters = 模型里所有可训练权重（如线性层权重、QKV 投影矩阵、embedding 权重等）。
    - 训练的本质：通过最小化 loss 更新这些参数，使模型逐渐学会“合理预测下一个 token”。
- GPT-2 vs GPT-3（为什么学习实现选 GPT-2）
    - GPT-3 更大、训练数据更多 → 但训练/复现需要 GPU 集群
    - GPT-2（尤其 small）更适合学习实现：预训练权重公开、规模可在单机/笔记本运行
- GPT_CONFIG_124M（配置的意义）
    - 它定义 GPT-2 small 的结构与行为：
    - vocab_size, context_length, emb_dim, n_heads, n_layers, drop_rate, qkv_bias ...
    - 逻辑：模型不是“写死结构”，而是靠 config 参数决定“有多大、多深、注意力怎么分头”等。
- DummyGPTModel（占位模型）是什么、为什么要
    - 它是“脚手架”：先把模块顺序与数据流搭好（embedding → blocks → output），
    - 确保张量形状、前向传播流程正确，再逐个替换成真正实现的组件。

### 4.2 LayerNorm：目的、位置、方差计算、与 BatchNorm 差异

- LayerNorm 目的（解决什么）
    - LayerNorm（LN）把某层激活规范到 均值 0、方差 1（沿特征维），主要用来：
        - 稳定训练、加速收敛
        - 降低梯度消失/爆炸风险
        - 让深层 Transformer 训练更可靠
- LN 放在哪（位置逻辑）
    - 在 GPT-2/现代 Transformer 中常见：
    - 在注意力/FFN 前后都用（尤其是 Pre-LN 结构里：先 LN 再进子层）
    - 模型输出前再做一次 final LayerNorm（让送进输出层的分布更稳定）
- 方差 biased vs unbiased（为什么 LLMS 用 biased）
    - biased：分母 n
    - unbiased：分母 n−1
    - 在大维度 n下差异很小，biased 常用来兼容 GPT-2/TensorFlow 默认实现。
- LayerNorm vs BatchNorm（为什么 LLM 偏爱 LN）
    - LN：沿 feature 维归一化（对每个样本独立）
    - BN：沿 batch 维归一化（依赖 batch 统计）
    - LLM 训练/推理常遇到 batch size 变化、甚至很小的 batch，因此 LN 更稳定、更灵活。
- 四个定义
    - Transformer Block：GPT 的核心结构块（masked MHA + FFN 顺序应用）
    - Layer Normalization：把层输出规范到均值 0、标准差 1
    - Parameters：可训练权重
    - Logits：softmax 前的未归一化输出（不是概率）

### 4.3 FeedForward + GELU：特性结构、缺失项、作用、维度一致性

- GELU 是什么（为什么不用 ReLU）
    - GELU 更平滑，近似 ReLU
    - 与RELU区别
        - ReLU："如果x>0就输出x，否则输出0"，负数直接变 0（梯度也 0）
        - GELU："根据x有多大于0，决定输出多少"，负输入通常仍保留小的非零输出/梯度 → 学习更细腻、训练更平滑
    - （所以 GPT/Transformer 常用 GELU）
- FeedForward 模块结构（它在 block 里干嘛）
    - 标准 FFN（逐位置 MLP）：
        - Linear（扩维） → GELU → Linear（收缩回原维度）
        - 逻辑：扩维提供更大表达空间 → 非线性让表示更丰富 → 收缩回原维度便于堆叠与残差。代码缺失项（Position 1 2 → E D）
- 为什么输入输出同维度
    - 为了：
    - 能把很多 block 一层层叠起来（每层输入输出 shape 一致）
    - 方便做残差连接（x + f(x) 要求维度匹配）

### 4.4 Shortcut connections：梯度消失 skip机制、验证、图示解释

- 梯度消失是什么（为什么深层难训）
    - 反向传播时梯度随层数变小 → 早期层更新不到位 → 学不到有效特征。
- Shortcut / skip 做了什么
    - 残差连接：
    - 它提供一条“绕开子层”的直接通路：
        - 前向：保留原信息，避免信号被过度变形
        - 反向：给梯度一条更直接的回传路径 → 缓解梯度消失
- 代码实现（use_shortcut）
    - 用开关控制是否启用残差，便于对比实验。
- print_gradients（如何证明 shortcut 有用）
    - 打印每层“平均绝对梯度”：
        - 无 shortcut：靠前层梯度可能趋近 0
        - 有 shortcut：各层梯度更均衡，早期层不至于“学不到”

图右侧梯度更大：因为 shortcut 阻止梯度在层间衰减到极小。

### 4.5 TransformerBlock：拼接、Pre-LN、shortcut作用、维度保持、上下文融合

- 一个 Transformer block 由什么组成
    - masked multi-head attention
    - feed forward（FFN）
    - layer norm
    - dropout
    - shortcut（残差）
- 每个组件在链路中的角色
    - attention：把“序列内部关系”融进每个 token 表示（上下文融合）
    - FFN：对每个位置做非线性变换，提升表达能力
    - LN：稳定数值尺度，训练更稳
    - dropout：防过拟合
    - shortcut：保梯度、保信息，深层更易训
- Pre-LayerNorm（为什么强调）
    - Pre-LN = 在 attention/FFN 之前做 LN。
    - 相比 Post-LN，通常训练更稳定、深层更好训（现代 GPT 系列常见）。
- 维度保持 & 上下文融合怎么同时成立
    - shape 不变：输入输出一一对应，适配序列任务
    - 内容变了：每个输出向量融合了全序列上下文信息（attention 的贡献）

### 4.6 GPTModel：组装、final LN、weight tying、logits → 文本、层数影响

- GPTModel 的组装顺序（信息流）
    - token + pos embeddings
    - → 多层 TransformerBlock
    - → final LayerNorm
    - → 输出投影到 vocab 得 logits（预测 next token）
- final LayerNorm 为什么要
    - blocks 叠很多层后，输出分布可能漂移；final LN 把尺度拉回稳定范围，提高性能与训练可靠性。
- weight tying（为什么这么做）
    - 复用 token embedding 权重作为输出层权重：
        - 减少参数量
        - 可能更快、更省内存
        - 经验上效果也常不错
- logits 的形状与含义
    - 输出张量形状：\[batch_size, num_tokens, vocab_size\]
    - 每个位置是一组 logits（对词表中每个 token 的“未归一化打分”）。
    - 生成文本：logits → softmax 得概率 → 选 token（argmax 或采样）→ decode 回文本。
- blocks 层数影响
    - 层数越多：
        - 参数更多、算力/显存更高
        - 表达能力更强，更能捕捉长程依赖 → 生成质量通常更好（但也更贵）

### 4.7 文本生成：softmax、生成循环、softmax 为何多余、greedy decoding、未训练原因、步骤顺序

- 生成的基本流程（从模型输出到文本）
    - 模型输出 logits
    - softmax → 概率分布
    - 选择下一个 token（argmax 或采样）
    - 把新 token 拼回上下文
    - 重复，直到生成指定长度
    - decode → 文本
- generate_text_simple（生成循环是什么）
    - 一个最小实现：不断用当前上下文预测下一 token，然后 append 回输入，循环生成。
- softmax technically redundant（为什么说“对 argmax 来说”softmax 多余）
    - softmax 是单调变换，不改变最大值位置：
        - argmax(logits) 和 argmax(softmax(logits)) 结果相同
        - 所以如果你只做 greedy decoding（每步取最大概率），softmax 对“选谁”不必要。（但如果要看概率、做采样/温度/Top-k/Top-p，就需要 softmax 或等价处理。）
- greedy decoding（优缺点）
    - 每步选最可能 token：
        - 优：简单、高效
        - 缺：容易重复、文本更可预测，缺少多样性
- 为什么未训练会胡言乱语
    - 因为参数是随机的，模型没有学到：
        - 词与词之间的统计规律
        - 上下文如何影响下一词
    - 训练才让它建立这些语言模式，生成才会连贯。

## 5 Pretraining on unlabeled data

本章围绕三种能力，形成一条从“评估→训练→可控生成→工程保存→加载预训练权重”的完整落地链路：

1.  **预训练与评估**：用训练/验证 loss（核心是 cross-entropy）衡量模型是否学到了“预测下一个 token”的能力，并用 **perplexity** 把 loss 变成更直观的指标。
2.  **可控生成**：通过 **temperature scaling** 与 **top-k sampling** 控制生成的随机性与多样性，对比 greedy 与概率采样；并用 **\-∞ masking** 把不合格候选彻底排除。
3.  **工程落地**：保存/加载 **model state_dict + optimizer state** 实现恢复训练；最后从 OpenAI 下载 GPT-2 预训练权重并加载进自定义 GPTModel，处理 config 兼容（context_length、qkv_bias）。

### 5.1 评估生成式文本模型：generate_text_simple、loss、反传、cross entropy、perplexity、图示标注、代码缺失

这一节让你建立最基本的“评估视角”：

- GPT 生成文本的质量，本质上来自它对 next token 的预测是否准确；
- 衡量准确性的核心就是 cross-entropy loss，并可用 perplexity 做更直观解释。
- generate_text_simple 工作流（从文本到文本，链路必须顺）
    - 起始文本 → token IDs → 送入 GPT → 输出 logits → 转 token IDs → 解码成文本。
    - 这条链路对应“推理/生成”的最小闭环。
- text generation loss & backpropagation（文本生成损失和反向传播）
- （评估如何连接到训练）（这部分参考神经网络架构）
    - loss：衡量预测 tokens 和目标 tokens 的差距；越低代表预测更接近真实（生成更可能连贯）。
    - 反向传播backpropagation：用 loss 计算梯度并更新权重；更新后的模型更倾向于给正确 token 更高概率，进而降低 loss。
    - 逻辑：你能评估（算 loss），才谈得上训练（反传降低 loss）。

你教AI写作文：它写完后，你对照范文打分（loss），然后告诉它哪里要改（backpropagation），它改进了下次就能写更好。

cross entropy loss（为什么它是核心）

- 它衡量“真实 token（one-hot 分布）”与“模型预测分布”的差距，是 LLM 训练/评估的主损失。
- 直觉：模型如果把真实 token 赋予高概率，交叉熵就低；赋予低概率就高。

perplexity（为什么引入它）

- perplexity 由交叉熵导出，更直观：
- 可以理解为每一步预测时的“有效不确定词表大小”
- 越低越好（模型越确定、越接近真实）
- 逻辑：cross-entropy 更像“优化指标”，perplexity 更像“可解释指标”。

图示标签（必须原样记住，不改写）

1 输入文本→词表→token IDs

2 每个输入向量对应 7 维概率行向量

3 预测 token IDs（最高概率索引位置）

4 argmax 得最高概率索引

5 索引→逆词表→文本

6 输入文本

7 LLM 生成输出文本

### 5.2 训练 LLM：train_model_simple、evaluate_model、缺失映射、生成样例、AdamW、loss 曲线与过拟合

把 5.1 的“评估闭环”升级为“训练闭环”：

- 通过训练循环持续降低训练 loss，同时用验证集 loss 监控泛化并识别过拟合。

- train_model_simple（基础训练循环）
    - epochs → batches → 算 loss → 更新权重 → 用验证集评估。
    - 这是最小可用训练框架。
- evaluate_model（为什么要单独写评估函数）
    - 评估时要保证结果可靠，所以需要：
        - 切到 eval()（禁用 dropout 等训练特性）
        - 用 no_grad()（不跟踪梯度，省内存更快）
        - 计算训练集与验证集 loss（用作诊断）
        - 逻辑：训练 ≠ 评估；评估要“干净”，否则 loss 会被 dropout 等因素扰动。

- generate_and_print_sample（为什么训练时要生成样例）
    - 训练 loss 降低不一定等于文本真的变好看；生成样例能直观看到：
        - 是否更连贯
        - 是否更少乱码
        - 是否出现模式化/重复
        - 它把“数值指标”与“人类直觉质量”连接起来。

- AdamW vs Adam（为什么 LLM 常用 AdamW）
    - AdamW 的关键是更合理的 weight decay（对大权重惩罚更干净），通常更利于：
        - 正则化
        - 减少过拟合
        - 提升泛化稳定性

- loss 曲线与过拟合（Figure 5.12 必会解释）
    - 训练 loss 继续下降，但验证 loss 停滞甚至上升
        - → 模型在记忆训练数据模式，而没有学到可泛化规律
        - → 结论：过拟合

### 5.3 解码策略：temperature、top-k、greedy vs sampling、Inf masking、generate 如何组合

把“模型会预测”变成“生成可控”：

- 同一个模型输出 logits，你可以用不同解码策略控制文本的确定性 vs 多样性。

temperature scaling（怎么影响随机性）

- 做法：logits / temperature
    - 温度 高：分布更均匀 → 随机性↑ → 多样性↑
    - 温度 低：分布更尖锐 → 更确定 → 更偏向最可能 token
    - 逻辑：它是在“同一组 logits”上调节“选择时敢不敢冒险”。

top-k sampling（为什么能减少胡话）

- 只保留 top-k 的 token 参与采样，把低概率尾部候选全部排除：
    - 减少不合语法/语义跳跃的低概率词
    - 同时保留一定随机性（比 greedy 多样）

greedy vs probabilistic sampling（必会对比）

- greedy：每步取最高概率 token → 最确定，但易重复、可预测
- sampling：按概率随机选 → 更丰富、更有创造性，但可能更不稳定

- Inf masking（选择题结论 + 逻辑）
    - 目的：把不在 top-k 的 logits 设为 -∞，确保它们被彻底排除（C）。
    - 逻辑：softmax(-∞)=0 → 真正“不可选”。
- generate 函数如何集成（组合规则）
    - 如果给 temperature → 先缩放 logits
    - 如果给 top-k → 再做 masking（排除 top-k 外 token）
        - 最后再根据策略采样/选择。

### 5.4 保存/加载权重：为什么保存、state_dict、eval 模式、为什么保存优化器

把训练从“跑一次就没了”升级为“可恢复、可复用、可部署”：

通过保存 checkpoint，让模型训练与推理具备工程可持续性。

为什么保存（核心动机）

- LLM 训练昂贵；保存 checkpoint 可以：
- 不必从零重训
- 随时恢复训练
- 复用到下游任务或部署

推荐方式：保存 state_dict

- 用 torch.save(model.state_dict())（或连同 optimizer 一起保存）。
- state_dict 是参数张量的标准容器，加载方便、兼容性强。

model.eval()（为什么推理要 eval）

- 进入评估/推理模式：
    - 禁用 dropout
    - 使输出稳定可复现（同输入→更一致输出）

为什么要保存 optimizer state（非常关键）

- 像 AdamW 这类优化器有“历史状态”（动量、二阶矩估计等）。
- 如果只加载模型权重，不加载 optimizer：
    - 恢复训练时会“失忆”
    - 学习过程可能变差或不收敛

### 5.5 从 OpenAI 加载 GPT-2 预训练权重：优势、settings/params、加载函数、model_configs、兼容更新

把你的“自定义 GPTModel”与“真实可用的大模型能力”接轨：

下载 GPT-2 预训练权重并加载到你实现的模型中，立刻获得更强生成能力，而不是从随机权重开始胡说。

主要优势（为什么这步重要）

- 省去大规模预训练：节省时间与算力，直接站在成熟模型基础上做实验/微调/评估。

settings vs params（必须分清）

- settings：模型架构设置（层数、维度、heads、context_length 等）
- params：每层具体权重张量（QKV 权重、FFN 权重、embedding 权重等）
- 逻辑：settings 决定“形状/结构”，params 才是“学到的能力”。

load_weights_into_gpt（它在做什么）

- 把 OpenAI 权重与自定义 GPTModel 的对应层一一匹配并加载。
- 关键点是“精确对应”，否则 shape 对不上或语义对不上。

model_configs（为什么需要它）

- GPT-2 有不同规模（small/medium/large/xl），每种规模的 settings 不同。
- model_configs 提供这些设置，便于你选定模型并加载对应权重。

为什么更新 NEW_CONFIG 的 context_length 与 qkv_bias

- 为了 兼容 OpenAI GPT-2 权重与实现细节：
    - OpenAI 预训练 GPT-2 的 context_length 与你默认 config 可能不同
    - GPT-2 注意力模块使用 bias（qkv_bias=True）

\- 所以必须更新这些配置，才能正确加载并运行。

## 6 Fine-tuning for classication

本章核心任务是**分类微调**：用一个预训练 GPT（已经学到通用语言知识）完成特定分类任务——**垃圾短信识别**。整体流程（你给的 8 步主线）可以连成一条工程闭环：

→（1）明确微调类型

→（2）准备**平衡**数据集并划分 train/val/test

→（3）解决变长文本 batching（padding/truncation）并用 DataLoader 批量化

→（4）加载预训练权重并用生成文本验证加载成功、建立 baseline

→（5）替换输出层为分类头，并设置冻结/解冻策略

→（6）用最后 token 输出做分类，定义 loss 与 accuracy

→（7）写微调训练循环与评估策略（含 eval_iter）

→（8）推理部署：classify_review 流水线与模型保存

### 6.1 微调类型：指令微调 vs 分类微调（泛用 vs 专用）

先确定“你到底在做哪种微调”，因为不同微调类型决定：

- 输出形式（自由文本 vs 固定标签）
- 数据组织方式
- 输出层结构与 loss 设计

两类最常见微调（定义 + 例子）

- Instruction fine-tuning（指令微调）：让模型更会理解并执行自然语言指令。

例：按提示把英文翻译成德语、按格式写总结、根据指令做推理等。Classification

- fine-tuning（分类微调）：让模型输出固定类别标签。

例：spam/ham、情感正负、主题分类；也类比到图像分类等。

分类微调的关键限制（为什么叫“专用”）

- 分类微调只能输出训练中见过的预定义类别，因此更像专用模型；
- 指令微调能适配更广泛的任务形式，更像通用/泛用模型。

何时选指令微调（应用判断）

- 当你需要处理多样任务、复杂指令、强调适应性与交互质量时 → 倾向指令微调；
- 当任务目标明确、标签固定、追求高准确率与稳定输出 → 倾向分类微调（本章选择）。

本节术语定义（匹配题要掌握）

Instruction Fine-Tuning：按指令训练提升理解/执行自然语言任务能力

Classification Fine-Tuning：训练模型识别固定类别标签

Generalist Model：多任务表现好

Specialized Model：针对特定任务高度专精

衔接到 6.2：确定了“分类微调”，下一步就必须准备带标签的数据集。

### 6.2 数据集准备：平衡、标签编码、划分、保存

把原始短信数据处理成“可训练的监督学习数据集”：

- 文本 + 标签（spam/ham）
- 类别分布合理
- 划分 train/val/test
- 保存方便复用

数据内容与任务

- 数据由短信文本与标签组成：spam / non-spam（ham），用于演示分类微调。

为什么要平衡（undersampling 的动机）

- 原始 spam 与 ham 通常不平衡；如果直接训练：
    - 模型可能“学会永远预测多数类”也能得到高表面准确率
    - 但对少数类（spam）识别能力差
- 因此用 欠采样（undersampling）把数据变成 1:1 平衡，避免模型偏向多数类。

标签编码（从字符串到整数）

- 把 ham/spam 这种字符串标签映射到整数 0/1：
- 类比文本→token IDs，但这里是“类别 ID”
- 训练时交叉熵一般需要整数类别标签

random_split：train/val/test 的意义

- train：用于更新权重（学习）
- val：调参、防过拟合、选择 epoch（训练过程内的“监控集”）
- test：最终评估泛化（训练完成后才使用）

保存 CSV（工程意义）

- 把处理好的数据保存为 CSV：
    - 方便反复实验与复现
    - 便于后续分析与继续训练

衔接到 6.3：数据准备好了，但短信长度不一，必须解决变长 batching 才能上 DataLoader。

### 6.3 DataLoader 与变长 batching：两种方案、padding token、图示标签、批结构与一致性规则

让变长短信能组成 batch 进入模型训练：

- 统一每个 batch 的张量形状
- 保证 train/val/test 的长度处理一致

变长 batching 的两种方案（优缺点）

- 全截断到最短：便宜但丢信息（长短信被砍得很严重）
- 全 padding 到最长：保留信息但多算力（短短信会补很多 padding）

本章采用“padding/truncation 结合”：

- 以某个最大长度为基准
- 短的补齐，长的截断

padding token=50256（必须记）

- 短短信用 padding token ID 50256 补齐到统一长度。
- 作用：让 batch 内每条短信长度一致。

SpamDataset 的职责（它为什么是关键封装）

- 读取 CSV
- 用 GPT-2 tokenizer 把文本转为 token IDs
- 对 token IDs 做 padding / truncation
- 提供 \__getitem__ 与长度接口，让 DataLoader 能批量取样

图示标签（必须按原映射记）

1 Tokenize text

2 Token Ids

3 Pad to longest sequence

4 Padded Token Ids

5 Longest message has no padding

验证/测试集长度规则（为什么要对齐训练）

- 验证/测试集必须按训练集最长序列长度进行 padding；超过则截断。
    - 目的：保证输入形状一致，评估可比，并避免模型在不同阶段看到不同长度分布导致偏差。

一个训练 batch 的明确结构（必须记的形状例子）

- 8 条短信 × 每条 120 tokens（token IDs 张量）
- 标签在另一个张量里（8 个标签）

DataLoader 的作用

- 高效按 batch 取数据
- 设置 batch_size、shuffle、workers
- 分别构建 train/val/test loaders

衔接到 6.4：数据管道就绪后，下一步是加载预训练 GPT 权重（否则从随机开始分类会更慢更差）。

### 6.4 预训练权重初始化：为什么先加载、怎么加载、如何用生成验证、为什么先用 spam prompt

用预训练权重给分类任务提供强起点，并确认加载成功。

为什么先加载预训练权重

- 预训练 GPT 已从海量无标注文本中学到通用语言规律（词义、语法、常识模式）。
- 分类微调只需在此基础上学习“把这些语言表征映射到 spam/ham 标签”，因此：
    - 收敛更快
    - 需要更少数据
    - 性能通常更好

加载流程（两步函数链）

- download_and_load_gpt2：按模型规模下载权重
- load_weights_into_gpt：把权重装入你的自定义 GPTModel

如何验证加载是否正确（为什么用生成验证）

- 给一个 prompt 用 generate_text_simple 生成文本：
    - 如果输出相对连贯，说明权重确实加载进去了
    - 如果仍是胡言乱语，可能是权重没加载对/配置不兼容

为什么先用 spam prompt（baseline）

- 在微调前观察模型对“spam”相关提示的初始生成表现：
    - 建立 baseline（初始能力）
    - 明确微调之后要提升的方向（分类准确率/判别能力）

衔接到 6.5：预训练模型是“生成头（词表输出）”，但分类需要“类别头”，所以必须换输出层并决定冻结策略。

### 6.5 分类头：替换输出层、输出节点=类别数、冻结/解冻、为什么选最后 token

把 GPT 从“预测下一个词”改造成“预测类别标签”。

为什么要替换输出层

- 预训练 GPT 的输出层是 词表预测头（vocab_size 维 logits），用于生成；
- 分类任务只需要 类别 logits（2 类或多类），因此要替换成分类头（小线性层）。

输出节点数=类别数（通用设计）

- 每个类别一个输出节点：
    - 二分类用 2 节点
    - 多分类可直接扩展
    - 无需为二分类单独换损失函数形式（仍用交叉熵很自然）。

为什么只微调最后几层通常足够

- 底层表示学到通用语言规律；越往上越任务相关。
- 因此只训练顶部（最后 block + 分类头）通常：
    - 更省算力、
    - 更少破坏通用知识
    - 在小数据任务上更稳

冻结/解冻怎么做

- requires_grad=False：冻结，不更新
- requires_grad=True：解冻，可更新

为什么用最后 token 做分类（因果 mask 关键逻辑）

- 在 GPT 的因果注意力下：
    - 第 t 个 token 只能看 1..t 的信息
    - 最后 token能看到整条序列，因此它的表示聚合信息最完整
    - 所以用“最后 token 的输出表示”作为分类依据最合适。

衔接到 6.6：有了分类头和“最后 token 表示”，下一步就要定义训练信号：loss 与 accuracy。

### 6.6 损失与准确率：argmax、softmax可选、calc_accuracy_loader、交叉熵原因、与语言建模 loss 差异

定义分类任务的“训练目标（loss）”与“评估指标（accuracy）”，并明确它和语言建模训练的区别。

分类预测怎么做（最后 token → 2 维 logits）

- 取最后 token 的类别 logits（维度=2），再：
    - argmax 得类别索引（spam/ham）

softmax 为什么“可选”

- softmax 把 logits 变成概率分布，但：
- argmax(logits) 与 argmax(softmax(logits)) 结果一致
    - 因此如果只是取类别，softmax 可省；
    - 但如果你要概率解释或阈值策略，则需要 softmax。

calc_accuracy_loader 做什么

- 遍历 dataloader：
    - 逐 batch 预测类别
    - 统计正确比例
    - 返回 accuracy（百分比）

为什么用交叉熵（关键：准确率不可微）

- accuracy 不可微，不能直接优化
- cross-entropy 可微，可用于反传更新参数
- 训练时最小化交叉熵通常会提升 accuracy（把正确类概率推高）。

与语言建模 loss 的本质区别

- 分类：只对最后 token 的输出算损失（一个标签对应整条短信）
- 语言建模：对所有 token 的 next-token 预测算损失（每个位置都有监督）

衔接到 6.7：有了 loss 与 accuracy，下一步就是写完整的微调训练循环，并用验证集监控泛化。

### 6.7 监督微调训练循环：与预训练区别、train_classifier_simple、evaluate_model、loss 曲线、epoch 选择、缺失映射、eval_iter

把第 5 章的训练框架改造成“分类微调框架”，并加入准确率评估与更轻量的评估策略。

微调 loop vs 预训练 loop（评价重点变化）

- 预训练更关注 token-level loss、生成样例；
- 分类微调的关键评估变成：
    - train/val loss
    - 分类 accuracy（更直观反映任务表现）
- train_classifier_simple 相比 train_model_simple 的主要改动
    - 统计训练样本数（不是 tokens）
    - 每 epoch 后计算 accuracy
    - 不再打印 sample text（生成不再是主目标）
- evaluate_model（仍然必要）
    - 计算 train/val loss：
        - 诊断是否过拟合
        - 观察训练进展是否真实改善泛化

Figure 6.16（必须会解释）

- 训练/验证 loss 初期快速下降 → 模型学到了有效决策特征
- 两者接近 → 泛化好、不过拟合
- （若训练降而验证不降/上升，则是过拟合信号）

epoch 选择（怎么判断）

- 数据/任务复杂：可能需要更多 epoch
- 观察验证 loss/accuracy：
    - 过拟合 → 减少 epoch 或加正则
    - 训练不足 → 增加 epoch

eval_iter（为什么要有它）

- 评估准确率时只跑部分 batch：
    - eval_iter 小 → 更快，但估计噪声更大
    - eval_iter 大 → 更准，但更耗时
    - 它是在“训练速度 vs 评估稳定性”之间做折中。

完整流程排序（必须原样保留，不改动）

1 A（加载 GPT-2 预训练权重）

2 C（冻结除输出层与最后 block 外所有参数）

3 G（替换输出层为两类线性层）

4 F（把新输出层与最后 block 的 requires_grad=True）

5 E（定义 loss/accuracy 计算函数）

6 D（初始化 AdamW）

7 B（用 train_loader 训练指定 epochs）

8 H（每 epoch 后在验证集评估）

衔接到 6.8：训练好后要进入部署视角：如何把一条新短信送入模型并输出类别，并把模型保存复用。

### 6.8 推理与部署：classify_review 流水线、准确率评估、阶段编号映射、保存模型、与预训练差异

把训练好的分类模型用于真实推理，并能保存/加载用于部署或后续实验。

使用微调模型分类（从输入到输出）

- 文本 → token IDs → 模型 → 输出类别 logits → 取最后 token → argmax → 返回 spam/ham。

classify_review（它做了哪些事）

- 预处理文本
- tokenize
- truncation（太长就截断）
- padding（补齐到长度一致）
- 加 batch 维度
- no_grad 推理
- 取最后 token logits
- argmax 得类别并返回类别名

测试集准确率（最终衡量）

- 在 test loader 上跑 accuracy，得到“对未见数据的泛化能力”。

推理阶段编号映射（必须原样记）

1 准备模型的输入数据

2 若序列过长则进行截断处理

3 将序列填充至最长序列长度

4 添加批次维度

5 模型进行无梯度跟踪的推理

6 获取最后输出词的逻辑值

7 返回分类结果

保存微调模型（为什么要）

- 部署实时检测（如短信过滤）
- 复现实验结果
- 继续训练/对比不同冻结策略与超参

分类微调 vs 预训练（最后再对比巩固）

相同点：

- 都是文本→token IDs
- 都用交叉熵作为可微训练信号

不同点：

- 预训练目标：预测下一个 token（词表输出头）
- 分类微调目标：预测类别标签（替换为分类头，只看最后 token）

## 7 Fine-tuning to follow instructions

预训练 LLM 的默认目标是 **completion（文本续写）**：给定一段文本，继续补下去。

- 但现实应用常需要它做明确任务：语法纠错、风格转换、指令问答等。预训练模型即使“语言能力强”，也可能：
- 不按指令格式输出
- 把任务当续写来胡编
- 对特定任务不稳定

因此本章引入 **instruction fine-tuning（指令微调）**：用大量“指令→理想回答”的监督数据，让模型学会**看到指令就输出目标回答**。并建立一条可复现管线：

**数据准备（JSON 指令数据） → prompt 格式化（Alpaca/Phi-3） → 变长 batching + loss 屏蔽（ignore padding） → DataLoader + device 管线（高效训练） → 选更大预训练底座建立 baseline → 微调训练（loss + 定性检查） → 在 test set 生成并抽取回答 → 人评/自动评估（LLM 打分 0–100） → 得到可量化提升指标**

### 7.1 指令微调为什么需要：从能力边界出发

明确“为什么必须做指令微调”：不是模型不聪明，而是它训练目标不匹配你的任务形式。

预训练 LLM 的主要能力

- 补全文本（completion）：学的是“下一个 token 应该是什么”。

常见挑战（能力边界）

- 当你给出明确指令（语法纠错、voice conversion、按格式输出等），预训练模型可能“挣扎”，因为：
    - 它默认把输入当作“要续写的前文”
    - 未必学会把“指令”当作强约束

指令微调的目的

- 提升模型遵循指令与输出目标回答的能力
- 更稳定地完成明确任务，而不是“随机续写”

关键前提（本节最重要落点）

- 要做指令微调，必须准备合适训练数据：大量 instruction → ideal output 的范例。

衔接到 7.2：所以下一节立即进入“数据格式与 prompt 设计”。

### 7.2 数据集准备：JSON 三字段结构 + 两种 prompt 风格（Alpaca vs Phi-3）

把“指令任务”变成可监督学习的数据结构，并把同一条数据格式化成模型可学的 prompt。

数据格式（必须记）

- JSON 文件
- 每条样本是 Python 字典对象，三字段：
    - instruction
    - input
    - output
- 训练监督来自：instruction/input → output（指令-回答对）。

两种 prompt 风格（为什么要区分）

(1) Alpaca 风格：结构化模板

- 显式区分 instruction / input / response
- 任务结构更清晰，适合教学与可读性强的指令数据

(2) Phi-3 风格：对话标记更简洁

- 用 &lt;|user|&gt; 与 &lt;|assistant|&gt; 这种标记 token
- 更像对话数据，模板短、贴近 chat 格式

format_input 的作用（本节的“关键函数”）

- 把 JSON 条目转换成指定风格的“模型输入字符串”：
- 如果是 Alpaca 风格，还会在输入里放 response 占位（让模型学习在 response 部分开始回答）
- 目标是让模型学到“指令部分→回答部分”的结构对应关系

数据集划分（train/val/test 的逻辑）

- train：用于参数更新
- val：训练中监控与调参（避免过拟合，挑 epoch）
- test：最终评估（held-out）

### 7.3 批处理与损失屏蔽：custom collate、batch 内最小 padding、targets 右移、ignore_index=-100、保留一个 eos

指令微调仍然是“预测下一个 token”的训练，但数据变长且包含 padding；必须：

- 高效 batching（减少 padding 浪费）
- 让 loss 忽略 padding（避免无意义监督）
- 确保目标序列正确（右移 + eos）

custom collate（为什么要自定义）

- 默认 collate 不知道怎么处理变长 token 序列。自定义 collate 做四件事：
    - batch 内最小 padding：只把同一个 batch 的样本 padding 到该 batch 的最长长度，减少全局 padding 浪费。
    - padding token：使用 &lt;|endoftext|&gt; token ID 50256 做 padding。
    - targets 右移（next-token 训练的核心）
        - 训练仍是语言建模形式：targets 由 inputs 右移一位得到：
            - 去掉首 token
            - 末尾补一个 eos（保证对齐）
    - loss 屏蔽（ignore_index=-100）
        - targets 中 padding 位置替换为 -100（ignore_index），这样交叉熵不会计算这些位置的 loss，避免 padding 干扰训练。

“保留一个 eos”的意义（end of text token）

- 目标序列保留一个 end-of-text token，让模型学会：
    - 在回答结束处输出终止标记
    - 表示“回答完成”，提升回答边界感

衔接到 7.4：既然 collate 已经负责 padding 与 targets，那么进一步把 device 搬运也放进去可以提升训练效率。

### 7.4 DataLoader 与 device 管线：在 collate 搬运数据、allowed_max_length 控制上下文

构建训练/验证/测试 DataLoader，并优化数据到设备的搬运流程，同时保证输入不超过模型上下文。

1.  在指令微调上下文中，custom_collate_fn 函数的目的是什么？
2.  解释在 custom_collate_fn 函数内将数据移动到目标设备，而不是在主训练循环中这样做的优势。
3.  在 custom_collate_fn 函数中是如何确定和使用设备设置的？
4.  在 customized_collate_fn 函数中，allowed_max_length 参数的用途是什么？
5.  描述使用 DataLoader 类为训练集、验证集和测试集创建数据加载器的过程。

把 device 搬运放进 collate（为什么更高效）

custom_collate_fn 用于 batch 化指令数据集，并在送入模型前把 input/target 张量搬到指定 device（CPU/GPU/MPS）。

- 好处：搬运发生在 DataLoader 的数据准备阶段（更像后台工作）
- 减少训练 loop 对 GPU 的阻塞等待 → 提高 GPU 利用率与吞吐

device 选择与 partial 固定参数

- 根据 GPU 或 MPS 可用性选择 device；用 functools.partial 预填 device 参数，确保 collate 一直用对设备。

allowed_max_length（为什么必须）

- 把序列截断到模型支持的最大上下文长度（如 GPT-2 的 context length），避免：输入超长导致 shape 不兼容或显存爆炸

训练不稳定

DataLoader 的关键参数（本节落地）

- 构建 train/val/test loaders，并设置：
    - batch_size
    - collate_fn
    - shuffle（通常只对 train）
    - drop_last
    - num_workers 等控制 batch、打乱与加载行为。

（对应 batching、collate、padding tokens、ignore index 的定义关系）衔接到 7.5：管线就绪后，需要选一个更合适的预训练底座并建立 baseline，不然模型容量可能限制指令跟随上限。

### 7.5 为什么选更大底座（gpt2-medium 355M）：能力上限 + baseline 对比 + 回答抽取 Loading a pretrained llm

解释“为什么升级底座”以及“如何在微调前建立 baseline 并正确提取回答文本”。

为什么小模型不够

- 指令跟随通常需要更复杂的模式与更强表达能力；小模型容量不足，可能：
    - 学不全任务分布
    - 输出不稳定
- 难以同时保持语言质量与遵循指令

- 更大模型（如 gpt2-medium 355M）参数更多，上限更高，更能学到稳定指令行为。

加载预训练模型的意义（复用语言知识）

- 预训练权重提供通用语言能力；微调只需“校准行为”到指令遵循→更高效、更少数据。
- 代码差异很小（工程要点）

从 gpt2-small 换成 gpt2-medium：规模改变但训练管线几乎不变。

微调前先评估 baseline（为什么必要）

- 在训练前先测预训练模型的表现：
- 量化微调带来的增益
- 防止“训练后变差却没意识到”
- 为后续评估提供对照组

抽取回答（必须掌握的处理）

- 生成文本通常包含“输入 prompt + 模型续写”。要评估回答质量，需要：
    - 按输入长度切掉输入部分
    - 只保留模型回答部分
    - strip() 去空白

衔接到 7.6：有了更大底座 + baseline + 数据管线，下一步就是实际微调训练，并处理资源限制与长度/批大小等约束。

### 7.6 微调训练与挑战：loss 曲线 + 定性查看回答；硬件/长度/批大小是核心约束

把模型训练成“按指令给正确回答”，并理解训练中最常见的瓶颈与诊断方式。

微调训练的基本逻辑

- 加载预训练模型 → 指令数据（prompt→target）→ 用 optimizer 最小化 loss
- 因为训练仍是 next-token 预测，所以 loss 下降通常意味着模型更能生成目标回答序列。

常见挑战（资源约束是主矛盾）

- 显存/内存限制（尤其大模型 + 长序列）
    - 应对策略：换小模型或更合理的模型规模
    - 降低 batch_size
    - 使用 GPU/MPS
    - 控制输入长度（allowed_max_length）

训练中评估的两条线（数值+定性缺一不可）

- 看 train/val loss 是否下降、是否分离（过拟合/不足）
- 在训练过程中定性检查生成回答：是否遵循指令、是否格式正确、是否胡编

Alpaca 数据集的意义（为什么有效）

- 提供大量多样指令—回答对，让模型学到更广的任务行为模式，提升泛化的指令跟随能力。

衔接到 7.7–7.8：训练完不能只看 loss，必须建立评估体系：生成 test 回答→抽取→保存→人评或自动评估，得到可量化指标。

### 7.7–7.8 评估体系：test 抽取回答→保存→人评/基准/自动化对话评估→LLM 打分 0–100 并求平均

（你把 7.7 与 7.8 合在一段主线里，我也按同样方式总结成一个“评估闭环”。）

- 评估流程：从 held-out test set 抽取模型生成回答 → 人工查看分析 → 再用多种方法量化质量（基准测试、人工偏好比较、自动化指标等）。

三类评估方式（各自的取舍）

- 短答案基准（如 MMLU）
    - 效率高、标准化强，但对对话细节/格式/指令遵循的覆盖有限。
- 人类偏好比较
    - 能捕捉细腻质量（nuance、语气、帮助性），但成本高、耗时。
- 自动化对话评估（如 AlpacaEval）
    - 规模化、高效，但可能缺少人类判断的细微差别。“对话表现”到底指什么（定义要会说）

对话表现

- 类人对话能力：理解上下文、把握意图、处理细节与 nuance。
- 对聊天/助手类应用很关键。

- 自动评估：用更大 LLM 打分（本章具体做法）
    - 用评估模型（如 Llama 3 / GPT-4 等）对回答质量打 0–100 分。

本章使用 Ollama 本地跑评估模型，通过 query_model 让评估模型给分。

替代评估模型：例如 **3.8B 的 phi3** 或 **70B 的 Llama 3**，具体选择取决于算力与期望性能。

generate_model_scores（核心评估函数）

- 遍历测试样本：
- 对每条样本调用评估模型评分
- 汇总并取平均
- 到一个定量指标（平均分）用于对比微调前后或不同设置

提升策略（评估驱动的迭代方向）

- 调参（学习率、batch、epoch、长度等）
- 扩充/多样化训练数据
- 换 prompt 风格/格式（Alpaca ↔ Phi-3）
- 使用更大预训练底座
